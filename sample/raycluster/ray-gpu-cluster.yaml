apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: cluster-gpu
#  labels:
#    ray.io/scheduler-name: volcano # the gang scheduler name, currently only support volcano
#    volcano.sh/queue-name: default # the queue name of volcano scheduler
  annotations:
    llmos.ai/volumeClaimTemplates: '[{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"name":"raycluster-gpu-head-log"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"2Gi"}}}}]'
    "ray.io/ft-enabled": "true"
spec:
  rayVersion: '2.31.0'
  enableInTreeAutoscaling: true # enable in-tree autoscaling
  # Ray head pod template
  headGroupSpec:
    # The `rayStartParams` are used to configure the `ray start` command.
    # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
    # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
    rayStartParams:
      block: "true"
      dashboard-host: "0.0.0.0"
      num-cpus: "0"
      redis-password: "$REDIS_PASSWORD"
    # Pod template
    template:
      spec:
        containers:
        # The Ray head container
        - name: ray-head
          image: anyscale/ray:2.31.0
          ports:
          - containerPort: 6379
            name: gcs
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          - containerPort: 8000
            name: serve
          resources:
            requests:
              cpu: "1"
              memory: "2Gi"
            limits:
              cpu: "2"
              memory: "4Gi"
          volumeMounts:
          - mountPath: /tmp/ray
            name: ray-logs
        volumes:
        - name: ray-logs
          persistentVolumeClaim:
            claimName: raycluster-gpu-head-log
  workerGroupSpecs:
  # the Pod replicas in this group typed worker
  - replicas: 1
    minReplicas: 0
    maxReplicas: 5
    groupName: small-group
    rayStartParams: {}
    template:
      spec:
        runtimeClassName: nvidia
        containers:
        - name: default-worker
          image: anyscale/ray:2.31.0
          resources:
            requests:
              cpu: "4"
              memory: "8Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "4"
              memory: "8Gi"
              nvidia.com/gpu: "1"
